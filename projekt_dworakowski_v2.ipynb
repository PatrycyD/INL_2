{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ce2e6c-6b00-4dcd-83ee-4fc9cdfae332",
   "metadata": {},
   "source": [
    "## Ze względu na znaczące trudności w doprowadzeniu do końca oraz otrzymania jakkkolwiek wartościowych wyników w poprzedniej wersji projektu (rozpoznawanie mowy - temat numer 5 z PolEval 2019) postanowiłem zmienić temat na analizę prześladowań w internecie we wpisach na portalu tweeter (temat numer 6 PolEval 2019) ze względu na to, że już mam doświadczenie w podobnych zadaniach dzięki czemu jestem w stanie bardziej świadomie przeprowadzić proces oraz prawidłowo zinterpretować wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773ae48-97e6-4127-9911-d3ed8ab9a800",
   "metadata": {},
   "source": [
    "#### W pierwszym etapie pobieram plik .zip udostępniony jako dane treningowe na stronie PolEval, rozpakowuję go i wczytuję zawartość (treść tweetów oraz etykiety) do zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e164e2-4f38-47ec-b4c4-834b5ffe2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "import zipfile\n",
    "import string\n",
    "import spacy\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import (TextVectorization, Dense, Embedding, LSTM, \n",
    "                                     Conv1D,  MaxPooling1D, GlobalMaxPooling1D,\n",
    "                                     Dropout, BatchNormalization, SpatialDropout1D,\n",
    "                                    Bidirectional)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, roc_auc_score\n",
    "import keras\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "zip_file_name = 'task_6-1.zip'\n",
    "tweets_contents_file = 'training_set_clean_only_text.txt'\n",
    "tweets_labels_file = 'training_set_clean_only_tags.txt'\n",
    "url = 'https://raw.githubusercontent.com/PatrycyD/INL_2/master/task_6-1.zip'\n",
    "if not os.path.isfile(zip_file_name):\n",
    "    wget.download(url) \n",
    "\n",
    "def extract_from_zip(zip_file, content_to_extract):\n",
    "    with zipfile.ZipFile(zip_file) as z:\n",
    "        with open(content_to_extract, 'wb') as f:\n",
    "            f.write(z.read(content_to_extract))\n",
    "            print('Extracted', content_to_extract)\n",
    "            f.close()\n",
    "        z.close()\n",
    "        \n",
    "def load_to_variable(file_to_load, data_type):\n",
    "    if data_type == 'np.array':\n",
    "        contents = np.array([]).reshape(1, -1)\n",
    "    else:\n",
    "        contents = []\n",
    "    with open(file_to_load, 'r', encoding='utf-8') as file:\n",
    "        for row in file:\n",
    "            if data_type == 'np.array':\n",
    "                contents = np.append(contents, row)\n",
    "            else:\n",
    "                contents.append(row)\n",
    "    file.close()\n",
    "    return contents\n",
    "\n",
    "\n",
    "if not os.path.isfile(tweets_labels_file):\n",
    "    extract_from_zip(zip_file_name, tweets_labels_file)\n",
    "\n",
    "if not os.path.isfile(tweets_contents_file):\n",
    "    extract_from_zip(zip_file_name, tweets_contents_file)\n",
    "    \n",
    "labels = load_to_variable(tweets_labels_file, 'np.array')\n",
    "labels = labels.astype(np.float32)\n",
    "tweets = load_to_variable(tweets_contents_file, 'list')\n",
    "# tweets = tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23555ed-6952-4d08-944c-5c45a8fbfd18",
   "metadata": {},
   "source": [
    "#### Poniżej zdefiniowane są funkcje służące do przeprocesowania tekstu. Wyliczając po kolei od góry służą do:\n",
    "- usunięcia słów 'stop words' - nie wnoszących wartości do tekstu\n",
    "- usuwanie linków http\n",
    "- usuwanie nazw użytkowników anonimowych\n",
    "- usuwanie emotek\n",
    "- usuwaniie znaków spoza tablicy ascii\n",
    "- usuwanie znaków interpunkcyjnych\n",
    "- usuwanie powtarzających się znaków\n",
    "- usuwanie cyfr\n",
    "- usuwanie nadmiarowych spacji\n",
    "- lematyzacja\n",
    "\n",
    "#### Następnie wszystkie te funkcje są wywoływane na wyżej wczytanych tekstach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a598a1e4-aa33-4aa8-9fb6-10c1301f9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stop words\n",
      "Cleaning URLs\n",
      "Removing nicknames\n",
      "Removing emojis\n",
      "Removing non ascii characters\n",
      "Removing punctuation\n",
      "Removing repeating characters\n",
      "Removing numbers\n",
      "Removing multiplied spaced\n",
      "Lemmatizing\n"
     ]
    }
   ],
   "source": [
    "stop_words = get_stop_words('polish')\n",
    "def remove_stop_words(text):\n",
    "    return ''.join([word for word in text if word not in stop_words])\n",
    "\n",
    "def clean_URLs(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_nicknames(text):\n",
    "    return text.replace('@anonymized_account', '').strip().strip('\\n')\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def remove_emojis(text):\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_non_ascii_chars(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text) \n",
    "\n",
    "punctuations_list = string.punctuation\n",
    "def clean_punctuation(text):\n",
    "    translator = str.maketrans('', '', punctuations_list) # jest to mapowanie i zamiana, dwa pierwsze argumenty to dwa stringi, gdzie znaki w pierwszym są zamieniane na znaki w drugim stringu, zgodnie z indeksem, Trzeci argument to znaki, które są mapowane do None => zostaną po prostu usunięte\n",
    "    return text.translate(translator)\n",
    "\n",
    "def clean_repeating_chars(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "\n",
    "def clean_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "def remove_multiplied_spaces(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "# !pip install --upgrade spacy\n",
    "# !python -m spacy download pl_core_news_sm\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_sentence = ''\n",
    "    for token in doc:\n",
    "        lemmatized_sentence = f'{lemmatized_sentence} {token.lemma_}'\n",
    "        \n",
    "    return lemmatized_sentence.strip()\n",
    "\n",
    "print('Removing stop words')\n",
    "tweets = [remove_stop_words(tweet) for tweet in tweets]\n",
    "print('Cleaning URLs')\n",
    "tweets = [clean_URLs(tweet) for tweet in tweets]\n",
    "print('Removing nicknames')\n",
    "tweets = [remove_nicknames(tweet) for tweet in tweets]\n",
    "print('Removing emojis')\n",
    "tweets = [remove_emojis(tweet) for tweet in tweets]\n",
    "print('Removing non ascii characters')\n",
    "tweets = [remove_non_ascii_chars(tweet) for tweet in tweets]\n",
    "print('Removing punctuation')\n",
    "tweets = [clean_punctuation(tweet) for tweet in tweets] \n",
    "print('Removing repeating characters')\n",
    "tweets = [clean_repeating_chars(tweet) for tweet in tweets]\n",
    "print('Removing numbers')\n",
    "tweets = [clean_numbers(tweet) for tweet in tweets]\n",
    "print('Removing multiplied spaced')\n",
    "tweets = [remove_multiplied_spaces(tweet) for tweet in tweets]\n",
    "print('Lemmatizing')\n",
    "tweets = [lemmatize(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74235d2-5171-4e08-98a8-ded925ea88d1",
   "metadata": {},
   "source": [
    "## Jeżeli tutaj pojawi się błąd spacy związany z pobieraniem paczki polskiej proszę o zresetowanie środowiska i uruchomienie wszystkiego od nowa - paczka może pobrać się prawidłowo po zaktualizowaniu spacy i zresetowaniu środowiska"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ae944-8c1b-4ff0-be2e-20f3d70163f0",
   "metadata": {},
   "source": [
    "#### Poniżej wczytuję przedtrenowany zbiór wektorów słów udostępniony przez Uniwersytet Łódzki. Zbiór ma ponad 930 000 wektorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481387de-573c-4a94-ab8c-ebde93f9b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 933198 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_file = 'pl-embeddings-cbow.txt'\n",
    "\n",
    "if not os.path.isfile(embedding_file):\n",
    "    url = 'http://publications.ics.p.lodz.pl/2016/word_embeddings/pl-embeddings-cbow.txt'\n",
    "    wget.download(url)\n",
    "    \n",
    "embeddings_index = {}\n",
    "with open(embedding_file, encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "embeddings_index.pop('933198') #na początku pliku jest notatka o liczbie wektorów i liczbie punktów w wektorach\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a12957-4643-4b18-8814-af180187492e",
   "metadata": {},
   "source": [
    "#### Poniżej pobieram dane testowe udostępnione na stronie PolEval i wczytuję je do odopwiednich zmiennych. Zrobię także preprocessing, tak żebym mógł użyć danych testowych do zrobienia słownika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f43305-a1b5-41f9-bce8-564ba8d19f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning URLs\n",
      "Removing nicknames\n",
      "Removing emojis\n",
      "Removing non ascii characters\n",
      "Removing punctuation\n",
      "Removing repeating characters\n",
      "Removing numbers\n",
      "Removing multiplied spaced\n",
      "Lemmatizing\n"
     ]
    }
   ],
   "source": [
    "def download_file_and_read_to_var(file_to_download, is_test):\n",
    "    url = f'https://raw.githubusercontent.com/PatrycyD/INL_2/master/{file_to_download}'\n",
    "    wget.download(url)\n",
    "    with open(file_to_download, 'r', encoding='utf-8') as file:\n",
    "        if is_test:\n",
    "            contents = np.array([]).reshape(1, -1)\n",
    "        else:\n",
    "            contents = []\n",
    "        for row in file:\n",
    "            if is_test:\n",
    "                contents = np.append(contents, int(row))\n",
    "            else:\n",
    "                contents.append(row)\n",
    "                \n",
    "        file.close()\n",
    "        \n",
    "    return contents\n",
    "        \n",
    "test_labels_file = 'test_set_clean_only_tags.txt'\n",
    "test_text_file = 'test_set_clean_only_text.txt'\n",
    "\n",
    "os.remove(test_labels_file)\n",
    "y_test = download_file_and_read_to_var(test_labels_file, True)\n",
    "    \n",
    "os.remove(test_text_file)\n",
    "X_test = download_file_and_read_to_var(test_text_file, False)\n",
    "\n",
    "def call_all_preprocessing_functions(text_data):\n",
    "    \n",
    "    text_data = [remove_stop_words(tweet) for tweet in text_data]\n",
    "    print('Cleaning URLs')\n",
    "    text_data = [clean_URLs(tweet) for tweet in text_data]\n",
    "    print('Removing nicknames')\n",
    "    text_data = [remove_nicknames(tweet) for tweet in text_data]\n",
    "    print('Removing emojis')\n",
    "    text_data = [remove_emojis(tweet) for tweet in text_data]\n",
    "    print('Removing non ascii characters')\n",
    "    text_data = [remove_non_ascii_chars(tweet) for tweet in text_data]\n",
    "    print('Removing punctuation')\n",
    "    text_data = [clean_punctuation(tweet) for tweet in text_data] \n",
    "    print('Removing repeating characters')\n",
    "    text_data = [clean_repeating_chars(tweet) for tweet in text_data]\n",
    "    print('Removing numbers')\n",
    "    text_data = [clean_numbers(tweet) for tweet in text_data]\n",
    "    print('Removing multiplied spaced')\n",
    "    text_data = [remove_multiplied_spaces(tweet) for tweet in text_data]\n",
    "    print('Lemmatizing')\n",
    "    text_data = [lemmatize(tweet) for tweet in text_data]\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "X_test = call_all_preprocessing_functions(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f701d-5547-4efe-a07f-b4d68ddc802d",
   "metadata": {},
   "source": [
    "#### Poniżej tworzę słownik wszystkich słów wraz z ich indeksami za pomocą kerasowego Vectorizera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53eb8fe2-96d4-4c07-9991-3cd6254b60c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16024, 40, 1153, 141]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 200000\n",
    "vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=200)\n",
    "vectorizer.adapt(tweets + X_test)\n",
    "\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "test = ['lipa', 'mecz', 'wyjazd', 'sezon']\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b333f-0694-480c-a763-144496089799",
   "metadata": {},
   "source": [
    "#### Poniżej tworzę macierz wszystkich słów występujących w danych zastąpnionych wektorami pobranymi z przedtrenowanego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032445b1-3867-4f86-a43f-223454c2902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 10426 words (9110 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, idx in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d372f-4702-4660-9991-df862bf6db04",
   "metadata": {},
   "source": [
    "#### Bardzo dużo słów nie znalazło odpowiedników w przedtrenowanym modelu. Najprawdopodoniej wynika to z faktu, iż występuje w tekście wiele wyrazów potocznych i mozliwe że specjalistycznych z zakresu piłki. Wiele osób może także popełniać błędy przy pisaniu i takich słów nie można przyporządkować. Może to obniżyć znacząco wyniki wytrenowanego modelu, jednak to najlepsze co udało mi się osiągnąć. Nie udało mi się także znaleźć większego polsiego przetrenowanego zbioru wektorów słów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354b1b9-b637-4492-b8b3-3da0f575bf37",
   "metadata": {},
   "source": [
    "#### Teraz przekronwertuję zbiór testowy do macierzy z wektorami słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e03ffa1-f470-401e-977f-4084b82fc337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing test data\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing test data')\n",
    "X_test = vectorizer(np.array([[s] for s in X_test])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec4ea5-9610-4ffa-9bf1-2902dddebc95",
   "metadata": {},
   "source": [
    "#### Poniżej przekonwertowałem do macierzy słów zbiór testowy oraz wydzieliłem z niego niewielki zbiór walidacyjny. Następnie stworzyłem trzy modele: lasów losowych, maszycn wspierających wektory i k najbliższych sąsiadów, następnie wytrenowałem je na przygotowanym zbiorze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630e2cb9-74f1-4466-995d-0f4044a9ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.15\n",
    "X_train = vectorizer(np.array([[s] for s in tweets])).numpy()\n",
    "X_train = X_train[:int(X_train.shape[0]*(1-val_size))]\n",
    "X_val = X_train[int(X_train.shape[0]*(1-val_size)):]\n",
    "y_train = np.array(labels)\n",
    "y_train = y_train[:int(y_train.shape[0]*(1-val_size))]\n",
    "y_val = y_train[int(y_train.shape[0]*(1-val_size)):]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=15, max_depth=5)\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "svc = SVC()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "rf_preds = rf.predict(X_test)\n",
    "knn_preds = knn.predict(X_test)\n",
    "svc_preds = svc.predict(X_test)\n",
    "\n",
    "rf_train_preds = rf.predict(X_train)\n",
    "knn_train_preds = knn.predict(X_train)\n",
    "svc_train_preds = svc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84da644-07e6-4dbc-8480-2cb23ddd0fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest test predictions\n",
      "[[866   0]\n",
      " [134   0]]\n",
      "\n",
      "K Nearest Neighbors test predictions\n",
      "[[866   0]\n",
      " [134   0]]\n",
      "\n",
      "Support Vector Machines test predictions\n",
      "[[866   0]\n",
      " [134   0]]\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest test predictions')\n",
    "print(confusion_matrix(y_test, rf_preds))\n",
    "print('\\nK Nearest Neighbors test predictions')\n",
    "print(confusion_matrix(y_test, knn_preds))\n",
    "print('\\nSupport Vector Machines test predictions')\n",
    "print(confusion_matrix(y_test, svc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11bd63a6-09d8-457c-a76f-21039ea6c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest train predictions\n",
      "[[7923    0]\n",
      " [ 611    0]]\n",
      "\n",
      "K Nearest Neighbors train predictions\n",
      "[[7923    0]\n",
      " [ 611    0]]\n",
      "\n",
      "Support Vector Machines train predictions\n",
      "[[7923    0]\n",
      " [ 610    1]]\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest train predictions')\n",
    "print(confusion_matrix(y_train, rf_train_preds))\n",
    "print('\\nK Nearest Neighbors train predictions')\n",
    "print(confusion_matrix(y_train, knn_train_preds))\n",
    "print('\\nSupport Vector Machines train predictions')\n",
    "print(confusion_matrix(y_train, svc_train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "855d704c-c2c2-4812-8240-cd4feeea02e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranom Forest test roc_auc: 0.5\n",
      "Ranom Forest train roc_auc: 0.5\n",
      "\n",
      "K Nearest Neighbors test roc_auc: 0.5\n",
      "K Nearest Neighbors train roc_auc: 0.5\n",
      "\n",
      "Support Vector Machines test roc_auc: 0.5\n",
      "Support Vector Machines train roc_auc: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(f'Ranom Forest test roc_auc: {round(roc_auc_score(y_test, rf_preds), 2)}')\n",
    "print(f'Ranom Forest train roc_auc: {round(roc_auc_score(y_train, rf_train_preds), 2)}')\n",
    "\n",
    "print(f'\\nK Nearest Neighbors test roc_auc: {round(roc_auc_score(y_test, knn_preds), 2)}')\n",
    "print(f'K Nearest Neighbors train roc_auc: {round(roc_auc_score(y_train, knn_train_preds), 2)}')\n",
    "\n",
    "print(f'\\nSupport Vector Machines test roc_auc: {round(roc_auc_score(y_test, svc_preds), 2)}')\n",
    "print(f'Support Vector Machines train roc_auc: {round(roc_auc_score(y_train, svc_train_preds), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b8557-fff4-4414-b3db-e19d503333ab",
   "metadata": {},
   "source": [
    "### Wyniki ze wszystkich trzech modeli nie są jakkolwiek przydatne, wszystko zostało zaklasyfikowane jako klasa pozytywna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea944efb-df23-4bc5-bc7d-00111d20e975",
   "metadata": {},
   "source": [
    "#### Stworzę teraz sieć neuronową w celu poprawy wyników. Zacznę od konwolucyjnej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005596e5-d213-41eb-b0f2-b4fa608d90f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         1953800   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, None, 100)        400       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         89728     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         114816    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               12900     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                3232      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,174,909\n",
      "Trainable params: 220,909\n",
      "Non-trainable params: 1,954,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "cnn = keras.Sequential()\n",
    "cnn.add(embedding_layer)\n",
    "cnn.add(BatchNormalization(epsilon=1e-05, momentum=0.1))\n",
    "cnn.add(Conv1D(128, 7, activation='relu', padding='same'))\n",
    "cnn.add(MaxPooling1D(2))\n",
    "cnn.add(Conv1D(128, 7, activation='relu', padding='same'))\n",
    "cnn.add(GlobalMaxPooling1D())\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(100, activation='relu'))\n",
    "cnn.add(Dense(32, activation='relu'))\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# adam = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "276575c7-b1bd-4805-9308-5852ab86940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "67/67 [==============================] - 18s 255ms/step - loss: 0.2949 - auc: 0.5422 - val_loss: 0.3497 - val_auc: 0.6995\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 17s 251ms/step - loss: 0.2481 - auc: 0.6684 - val_loss: 0.2906 - val_auc: 0.8554\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 19s 278ms/step - loss: 0.2183 - auc: 0.7916 - val_loss: 0.2380 - val_auc: 0.9283\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 16s 232ms/step - loss: 0.1821 - auc: 0.8780 - val_loss: 0.1662 - val_auc: 0.9701\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 15s 226ms/step - loss: 0.1377 - auc: 0.9427 - val_loss: 0.1216 - val_auc: 0.9864\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 15s 227ms/step - loss: 0.1068 - auc: 0.9642 - val_loss: 0.0584 - val_auc: 0.9981\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 15s 230ms/step - loss: 0.0824 - auc: 0.9753 - val_loss: 0.0459 - val_auc: 0.9995\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 15s 223ms/step - loss: 0.0676 - auc: 0.9834 - val_loss: 0.0292 - val_auc: 0.9996\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 16s 232ms/step - loss: 0.0441 - auc: 0.9932 - val_loss: 0.0206 - val_auc: 0.9998\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 15s 226ms/step - loss: 0.0355 - auc: 0.9953 - val_loss: 0.0343 - val_auc: 0.9999\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 15s 226ms/step - loss: 0.0304 - auc: 0.9946 - val_loss: 0.0100 - val_auc: 0.9997\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 15s 228ms/step - loss: 0.0301 - auc: 0.9937 - val_loss: 0.0144 - val_auc: 0.9998\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 18s 272ms/step - loss: 0.0326 - auc: 0.9945 - val_loss: 0.0138 - val_auc: 0.9998\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 19s 275ms/step - loss: 0.0259 - auc: 0.9943 - val_loss: 0.0044 - val_auc: 1.0000\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 19s 281ms/step - loss: 0.0290 - auc: 0.9957 - val_loss: 0.0257 - val_auc: 0.9997\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 18s 265ms/step - loss: 0.0206 - auc: 0.9989 - val_loss: 0.0092 - val_auc: 0.9999\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 19s 275ms/step - loss: 0.0125 - auc: 0.9980 - val_loss: 0.0069 - val_auc: 1.0000\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 19s 283ms/step - loss: 0.0141 - auc: 0.9988 - val_loss: 0.0060 - val_auc: 1.0000\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 19s 282ms/step - loss: 0.0165 - auc: 0.9969 - val_loss: 0.0029 - val_auc: 1.0000\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 21s 314ms/step - loss: 0.0090 - auc: 0.9990 - val_loss: 0.0038 - val_auc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23e0404db20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(X_train, y_train, epochs=20, validation_data=[X_val, y_val], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eae22d78-8006-4c18-a2e7-c534988a8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja do wyliczenia klas z wyników prawdopodobieństw wychodzących z sieci\n",
    "def return_classes_from_predictions(predictions):\n",
    "    preds_classes = np.array([]).reshape(1,-1)\n",
    "    for pred in predictions:\n",
    "        if pred[0] >= 0.5:\n",
    "            preds_classes = np.append(preds_classes, 1.)\n",
    "        else:\n",
    "            preds_classes = np.append(preds_classes, 0.)\n",
    "            \n",
    "    return preds_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ed82c5-2c8c-477f-b23f-bf0252dc8b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN train predictions\n",
      "[[7921    2]\n",
      " [   2  609]]\n",
      "\n",
      "CNN test predictions\n",
      "[[862   4]\n",
      " [125   9]]\n",
      "\n",
      "CNN test roc_auc: 0.53\n",
      "\n",
      "CNN test accuracy: 0.87\n",
      "CNN train roc_auc: 1.0\n",
      "CNN train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "cnn_preds = cnn.predict(X_test)\n",
    "cnn_preds_classes = return_classes_from_predictions(cnn_preds)\n",
    "cnn_train_preds = cnn.predict(X_train)\n",
    "cnn_train_preds_classes = return_classes_from_predictions(cnn_train_preds)\n",
    "\n",
    "print('CNN train predictions')\n",
    "print(confusion_matrix(y_train, cnn_train_preds_classes))\n",
    "\n",
    "print('\\nCNN test predictions')\n",
    "print(confusion_matrix(y_test, cnn_preds_classes))\n",
    "\n",
    "print(f'\\nCNN test roc_auc: {round(roc_auc_score(y_test, cnn_preds_classes), 2)}')\n",
    "print(f'CNN test accuracy: {round(accuracy_score(y_test, cnn_preds_classes), 2)}')\n",
    "print(f'\\nCNN train roc_auc: {round(roc_auc_score(y_train, cnn_train_preds_classes), 2)}')\n",
    "print(f'CNN train accuracy: {round(accuracy_score(y_train, cnn_train_preds_classes), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d408b8fa-ba86-46c5-a9a5-f05c005ebfba",
   "metadata": {},
   "source": [
    "#### Wyniki są zauważalnie lepsze. Wydaje się jednak że może być jakieś pole do poprawy. Spróbuję teraz zbudować sieć rekurencyjną i osiągnąć za jej pomocą lepsze wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b6db0ce-640b-4e0c-b16b-44824dde9d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         1953800   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, None, 128)        84480     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,083,721\n",
      "Trainable params: 129,921\n",
      "Non-trainable params: 1,953,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = keras.Sequential()\n",
    "rnn.add(embedding_layer)\n",
    "# rnn.add(LSTM(100))\n",
    "# # rnn.add(SpatialDropout1D(0.4))\n",
    "# rnn.add(Dropout(0.6))\n",
    "# rnn.add(Dense(128, activation='relu'))\n",
    "# rnn.add(Dense(1, activation='sigmoid'))\n",
    "rnn.add(Bidirectional(LSTM(64,  return_sequences=True)))\n",
    "rnn.add(Bidirectional(LSTM(32)))\n",
    "rnn.add(Dense(64, activation='relu'))\n",
    "rnn.add(Dropout(0.5))\n",
    "rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn.compile(loss='binary_crossentropy', optimizer='adam',  metrics=['AUC'])\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f30b47ae-06a4-423b-b1c1-af296c508c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "67/67 [==============================] - 58s 774ms/step - loss: 0.3056 - auc: 0.5148 - val_loss: 0.3547 - val_auc: 0.6706\n",
      "Epoch 2/5\n",
      "67/67 [==============================] - 53s 785ms/step - loss: 0.2381 - auc: 0.7039 - val_loss: 0.2779 - val_auc: 0.8763\n",
      "Epoch 3/5\n",
      "67/67 [==============================] - 52s 782ms/step - loss: 0.1795 - auc: 0.8778 - val_loss: 0.1865 - val_auc: 0.9452\n",
      "Epoch 4/5\n",
      "67/67 [==============================] - 52s 780ms/step - loss: 0.1343 - auc: 0.9357 - val_loss: 0.1227 - val_auc: 0.9758\n",
      "Epoch 5/5\n",
      "67/67 [==============================] - 58s 863ms/step - loss: 0.0888 - auc: 0.9725 - val_loss: 0.0738 - val_auc: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23e119786a0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train, epochs=5, validation_data=[X_val, y_val], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be2eee-6c05-477a-a19a-b9dbe24a1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_preds = rnn.predict(X_test)\n",
    "rnn_preds_classes = return_classes_from_predictions(rnn_preds)\n",
    "rnn_train_preds = rnn.predict(X_train)\n",
    "rnn_train_preds_classes = return_classes_from_predictions(rnn_train_preds)\n",
    "\n",
    "print('RNN train predictions')\n",
    "print(confusion_matrix(y_train, rnn_train_preds_classes))\n",
    "\n",
    "print('\\nRNN test predictions')\n",
    "print(confusion_matrix(y_test, rnn_preds_classes))\n",
    "\n",
    "print(f'\\nRNN test roc_auc: {round(roc_auc_score(y_test, rnn_preds_classes), 2)}')\n",
    "print(f'RNN test accuracy: {round(accuracy_score(y_test, rnn_preds_classes), 2)}')\n",
    "print(f'\\nRNN train roc_auc: {round(roc_auc_score(y_train, rnn_train_preds_classes), 2)}')\n",
    "print(f'RNN train accuracy: {round(accuracy_score(y_train, rnn_train_preds_classes), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4503672-b40d-446b-b75f-73cadfb80bb2",
   "metadata": {},
   "source": [
    "#### Wyniki z sieci rekurencyjnej są gorsze niż wyniki z sieci konwolucyjnej, dlatego należy uznać ją za najlepszy model. Możliwość poprawiania wyników może być ograniczona przez fakt dużej liczby słów które nie znalazły swoich odpowiedników w przedtrenowanym rozwiązaniu oraz ze względu na niewielki zbiór treningowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d87e8-2be3-4580-8520-59f22190c603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
