{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ce2e6c-6b00-4dcd-83ee-4fc9cdfae332",
   "metadata": {},
   "source": [
    "#### Ze względu na znaczące trudności w doprowadzeniu do końca oraz otrzymania jakkkolwiek wartościowych wyników w poprzedniej wersji projektu (rozpoznawanie mowy - temat numer 5 z PolEval 2019) postanowiłem zmienić temat na analizę prześladowań w internecie we wpisach na portalu tweeter (temat numer 6 PolEval 2019) ze względu na to, że już mam doświadczenie w podobnych zadaniach dzięki czemu jestem w stanie bardziej świadomie przeprowadzić proces oraz prawidłowo zinterpretować wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e164e2-4f38-47ec-b4c4-834b5ffe2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "import zipfile\n",
    "import string\n",
    "import spacy\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import TextVectorization, Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import keras\n",
    "\n",
    "\n",
    "zip_file_name = 'task_6-1.zip'\n",
    "tweets_contents_file = 'training_set_clean_only_text.txt'\n",
    "tweets_labels_file = 'training_set_clean_only_tags.txt'\n",
    "url = 'https://raw.githubusercontent.com/PatrycyD/INL_2/master/task_6-1.zip'\n",
    "if not os.path.isfile(zip_file_name):\n",
    "    wget.download(url) \n",
    "\n",
    "def extract_from_zip(zip_file, content_to_extract):\n",
    "    with zipfile.ZipFile(zip_file, encoding='utf-8') as z:\n",
    "        with open(content_to_extract, 'wb', encoding='utf-8') as f:\n",
    "            f.write(z.read(content_to_extract))\n",
    "            print('Extracted', content_to_extract)\n",
    "            f.close()\n",
    "        z.close()\n",
    "        \n",
    "def load_to_variable(file_to_load, data_type):\n",
    "    if data_type == 'np.array':\n",
    "        contents = np.array([]).reshape(1, -1)\n",
    "    else:\n",
    "        contents = []\n",
    "    with open(file_to_load, 'r', encoding='utf-8') as file:\n",
    "        for row in file:\n",
    "            if data_type == 'np.array':\n",
    "                contents = np.append(contents, row)\n",
    "            else:\n",
    "                contents.append(row)\n",
    "    file.close()\n",
    "    return contents\n",
    "\n",
    "\n",
    "if not os.path.isfile(tweets_labels_file):\n",
    "    extract_from_zip(zip_file_name, tweets_labels_file)\n",
    "\n",
    "if not os.path.isfile(tweets_contents_file):\n",
    "    extract_from_zip(zip_file_name, tweets_contents_file)\n",
    "    \n",
    "labels = load_to_variable(tweets_labels_file, 'np.array')\n",
    "labels = labels.astype(np.float32)\n",
    "tweets = load_to_variable(tweets_contents_file, 'list')\n",
    "# tweets = tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a598a1e4-aa33-4aa8-9fb6-10c1301f9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stop words\n",
      "Cleaning URLs\n",
      "Removing nicknames\n",
      "Removing emojis\n",
      "Removing non ascii characters\n",
      "Removing punctuation\n",
      "Removing repeating characters\n",
      "Removing numbers\n",
      "Lemmatizing\n"
     ]
    }
   ],
   "source": [
    "stop_words = get_stop_words('polish')\n",
    "def remove_stop_words(text):\n",
    "    return ''.join([word for word in text if word not in stop_words])\n",
    "\n",
    "def clean_URLs(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_nicknames(text):\n",
    "    return text.replace('@anonymized_account', '').strip().strip('\\n')\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def remove_emojis(text):\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_non_ascii_chars(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text) \n",
    "\n",
    "punctuations_list = string.punctuation\n",
    "def clean_punctuation(text):\n",
    "    translator = str.maketrans('', '', punctuations_list) # jest to mapowanie i zamiana, dwa pierwsze argumenty to dwa stringi, gdzie znaki w pierwszym są zamieniane na znaki w drugim stringu, zgodnie z indeksem, Trzeci argument to znaki, które są mapowane do None => zostaną po prostu usunięte\n",
    "    return text.translate(translator)\n",
    "\n",
    "def clean_repeating_chars(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "\n",
    "def clean_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "# !pip install --upgrade spacy\n",
    "# !python -m spacy download pl_core_news_sm\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_sentence = ''\n",
    "    for token in doc:\n",
    "        lemmatized_sentence = f'{lemmatized_sentence} {token.lemma_}'\n",
    "        \n",
    "    return lemmatized_sentence.strip()\n",
    "\n",
    "print('Removing stop words')\n",
    "tweets = [remove_stop_words(tweet) for tweet in tweets]\n",
    "print('Cleaning URLs')\n",
    "tweets = [clean_URLs(tweet) for tweet in tweets]\n",
    "print('Removing nicknames')\n",
    "tweets = [remove_nicknames(tweet) for tweet in tweets]\n",
    "print('Removing emojis')\n",
    "tweets = [remove_emojis(tweet) for tweet in tweets]\n",
    "print('Removing non ascii characters')\n",
    "tweets = [remove_non_ascii_chars(tweet) for tweet in tweets]\n",
    "print('Removing punctuation')\n",
    "tweets = [clean_punctuation(tweet) for tweet in tweets] \n",
    "print('Removing repeating characters')\n",
    "tweets = [clean_repeating_chars(tweet) for tweet in tweets]\n",
    "print('Removing numbers')\n",
    "tweets = [clean_numbers(tweet) for tweet in tweets]\n",
    "print('Lemmatizing')\n",
    "tweets = [lemmatize(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74235d2-5171-4e08-98a8-ded925ea88d1",
   "metadata": {},
   "source": [
    "### Jeżeli tutaj pojawi się błąd spacy związany z pobieraniem paczki polskiej proszę o zresetowanie środowiska i uruchomienie wszystkiego od nowa - paczka może pobrać się prawidłowo po zaktualizowaniu spacy i zresetowaniu środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481387de-573c-4a94-ab8c-ebde93f9b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 933198 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_file = 'pl-embeddings-cbow.txt'\n",
    "\n",
    "if not os.path.isfile(embedding_file):\n",
    "    url = 'http://publications.ics.p.lodz.pl/2016/word_embeddings/pl-embeddings-cbow.txt'\n",
    "    wget.download(url)\n",
    "    \n",
    "embeddings_index = {}\n",
    "with open(embedding_file, encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "embeddings_index.pop('933198') #na początku pliku jest notatka o liczbie wektorów i liczbie punktów w wektorach\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53eb8fe2-96d4-4c07-9991-3cd6254b60c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15057, 43, 1043, 139]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "vectorizer.adapt(tweets)\n",
    "\n",
    "# output = vectorizer([['kibic legia mioduski poznań wygrana przegrana']])\n",
    "# print(output.numpy()[0, :6])\n",
    "\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "test = ['lipa', 'mecz', 'wyjazd', 'sezon']\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032445b1-3867-4f86-a43f-223454c2902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 9936 words (8480 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, idx in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630e2cb9-74f1-4466-995d-0f4044a9ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer(np.array([[s] for s in tweets])).numpy()\n",
    "y_train = np.array(labels)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=15, max_depth=5)\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "svc = SVC()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# rf_preds = rf.predict(X_test)\n",
    "# knn_preds = knn.predict(X_test)\n",
    "# svc_preds = svc.predict(X_test)\n",
    "\n",
    "rf_train_preds = rf.predict(X_train)\n",
    "knn_train_preds = knn.predict(X_train)\n",
    "svc_train_preds = svc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "881588be-bf49-4f75-bc65-2b68a9d21d3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"There is no item named 'Task6\\\\task 01\\\\test_set_clean_only_tags.txt' in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b7002a75452e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtest_labels_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test_set_clean_only_tags.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtest_text_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test_set_clean_only_text.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mextract_test_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-b7002a75452e>\u001b[0m in \u001b[0;36mextract_test_files\u001b[1;34m(zip_file, file_name_to_extract)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name_to_extract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Task6\\task 01\\test_set_clean_only_tags.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Extracted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name_to_extract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, name, pwd)\u001b[0m\n\u001b[0;32m   1473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m         \u001b[1;34m\"\"\"Return file bytes for name.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1475\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1476\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[0;32m   1512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m             \u001b[1;31m# Get info object for name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m             \u001b[0mzinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mgetinfo\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1439\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNameToInfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m             raise KeyError(\n\u001b[0m\u001b[0;32m   1442\u001b[0m                 'There is no item named %r in the archive' % name)\n\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"There is no item named 'Task6\\\\task 01\\\\test_set_clean_only_tags.txt' in the archive\""
     ]
    }
   ],
   "source": [
    "test_file = 'task6_test.zip'\n",
    "\n",
    "if not os.path.isfile(test_file):\n",
    "    url = 'http://publications.ics.p.lodz.pl/2016/word_embeddings/pl-embeddings-cbow.txt'\n",
    "    wget.download(url)\n",
    "\n",
    "def extract_test_files(zip_file, file_name_to_extract):\n",
    "    with zipfile.ZipFile(zip_file) as z:\n",
    "        with open(file_name_to_extract, 'wb') as f:\n",
    "            f.write(z.read('Task6\\task 01\\test_set_clean_only_tags.txt'))\n",
    "            print('Extracted', file_name_to_extract)\n",
    "            f.close()\n",
    "        z.close()\n",
    "\n",
    "test_labels_file = 'test_set_clean_only_tags.txt'\n",
    "test_text_file = 'test_set_clean_only_text.txt'\n",
    "extract_test_files(test_file, test_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84da644-07e6-4dbc-8480-2cb23ddd0fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest test predictions\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-45644ffa157e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Random Forest test predictions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\nK Nearest Neighbors test predictions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknn_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\nSupport Vector Machines test predictions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "print('Random Forest test predictions')\n",
    "print(confusion_matrix(y_test, rf_preds))\n",
    "print('\\n\\nK Nearest Neighbors test predictions')\n",
    "print(confusion_matrix(y_test, knn_preds))\n",
    "print('\\n\\nSupport Vector Machines test predictions')\n",
    "print(confusion_matrix(y_test, svc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd63a6-09d8-457c-a76f-21039ea6c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest train predictions')\n",
    "print(confusion_matrix(y_train, rf_train_preds))\n",
    "print('\\n\\nK Nearest Neighbors train predictions')\n",
    "print(confusion_matrix(y_train, knn_train_preds))\n",
    "print('\\n\\nSupport Vector Machines train predictions')\n",
    "print(confusion_matrix(y_train, svc_train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d704c-c2c2-4812-8240-cd4feeea02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ranom Forest test f1: {round(f1_score(y_test, rf_preds)2)}')\n",
    "print(f'Ranom Forest train f1: {round(f1_score(y_train, rf_train_preds), 2)}')\n",
    "\n",
    "print(f'\\nK Nearest Neighbors test f1: {round(f1_score(y_test, knn_preds), 2)}')\n",
    "print(f'K Nearest Neighbors train f1: {round(f1_score(y_train, knn_train_preds), 2)}')\n",
    "\n",
    "print(f'\\nSupport Vector Machines test f1: {round(f1_score(y_test, svc_preds), 2)}')\n",
    "print(f'Support Vector Machines train f1: {round(f1_score(y_train, svc_train_preds), 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005596e5-d213-41eb-b0f2-b4fa608d90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = keras.layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = keras.layers.MaxPooling1D(5)(x)\n",
    "x = keras.layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = keras.layers.MaxPooling1D(5)(x)\n",
    "x = keras.layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "preds = keras.layers.Dense(1, activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276575c7-b1bd-4805-9308-5852ab86940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer='adam', metrics=['acc']\n",
    ")\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f6e5903-a76b-4b06-9da8-feea0cdd13ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-42c4d9658a62>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-39-42c4d9658a62>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    LSTM(150)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "embedding_size=32\n",
    "\n",
    "model = Sequential(\n",
    "                [\n",
    "                Embedding(vocabulary_size, embedding_size, input_length=max_words)\n",
    "                LSTM(150)\n",
    "                LSTM(100)\n",
    "                Dense(1, activation='sigmoid')\n",
    "                print(model.summary())\n",
    "                ]\n",
    ")\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "027947ac-2df6-49ac-bc85-4c713a0c9fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10041, 200)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aafca82d-835d-4582-8d51-fa0989aebbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10041"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f4d1541-a567-4e2d-a228-f52b826275ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18418, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0178eb31-90ab-48b5-b73d-e49e2f048d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18418"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "131ce7f0-3512-4d8e-a680-8b294735043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae22d78-8006-4c18-a2e7-c534988a8358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
